{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "(108, 3)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code adapted from:\n",
    "# https://github.com/FelixChop/MediumArticles/blob/master/LDA-BBC.ipynb\n",
    "\n",
    "# load in data\n",
    "data: pd.DataFrame = pd.read_csv('articlesData-ukraine.csv')\n",
    "data = data.dropna().reset_index(drop=True)\n",
    "data.shape\n",
    "### FINISH THIS"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<bound method NDFrame.head of                                                  title  \\\n0                      DEC Ukraine Humanitarian Appeal   \n1                                       War In Ukraine   \n2              Ukraine: Reporting from the Front Lines   \n3                                 The Ukraine Invasion   \n4                                  Letter from Ukraine   \n..                                                 ...   \n103  Ukraine war: London cab driver leads convoy to...   \n104  Ukraine: Aberdeen heating network terminates G...   \n105  Ukraine daily roundup: World leaders show unit...   \n106  Ukraine war: A dangerous escape on the 'Rescue...   \n107                                      lastUpdated:    \n\n                                               summary  \\\n0    An appeal on behalf of the DEC for the humanit...   \n1    BBC News reports on the latest developments in...   \n2    The BBC's teams report from the front lines on...   \n3    Yalda Hakim is in Lviv with the latest on the ...   \n4    Ukrainian writer Andrey Kurkov gives a persona...   \n..                                                 ...   \n103  Black cab driver Matt Westfall says \"there are...   \n104  Aberdeen Heat and Power is ending its contract...   \n105  It was a busy day of international diplomacy o...   \n106  A risky night-time race to safety lies ahead f...   \n107                         2022-04-01 08:14:43.297597   \n\n                                              contents  \n0                                              Unknown  \n1                                              Unknown  \n2                                              Unknown  \n3                                              Unknown  \n4                                              Unknown  \n..                                                 ...  \n103  A London taxi driver who led a convoy of black...  \n104  An Aberdeen heating network is terminating its...  \n105  It was a busy day of international diplomacy w...  \n106  Ten million people have fled their homes in Uk...  \n107                            with userQuery: ukraine  \n\n[108 rows x 3 columns]>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'progress_map'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [17]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# tokenise\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentences\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtitle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprogress_map\u001B[49m(sent_tokenize)\n\u001B[0;32m      4\u001B[0m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentences\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mhead(\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mtolist()\n\u001B[0;32m      6\u001B[0m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtokens_sentences\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentences\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mprogress_map(\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;28;01mlambda\u001B[39;00m sentences: [word_tokenize(sentence)] \u001B[38;5;28;01mfor\u001B[39;00m sentence \u001B[38;5;129;01min\u001B[39;00m sentences)\n",
      "File \u001B[1;32mC:\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:5583\u001B[0m, in \u001B[0;36mNDFrame.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   5576\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m   5577\u001B[0m     name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_internal_names_set\n\u001B[0;32m   5578\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_metadata\n\u001B[0;32m   5579\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m name \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_accessors\n\u001B[0;32m   5580\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_info_axis\u001B[38;5;241m.\u001B[39m_can_hold_identifiers_and_holds_name(name)\n\u001B[0;32m   5581\u001B[0m ):\n\u001B[0;32m   5582\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m[name]\n\u001B[1;32m-> 5583\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mobject\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__getattribute__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'Series' object has no attribute 'progress_map'"
     ]
    }
   ],
   "source": [
    "# tokenise\n",
    "\n",
    "data['sentences'] = data.title.progress_map(sent_tokenize)\n",
    "data['sentences'].head(1).tolist()\n",
    "\n",
    "data['tokens_sentences'] = data['sentences'].progress_map(\n",
    "    lambda sentences: [word_tokenize(sentence)] for sentence in sentences)\n",
    "data['POS_tokens'] = data['tokens_sentences'].progress_map(\n",
    "    lambda tokens_sentences: [pos_tag(tokens) for tokens in tokens_sentences])\n",
    "print(data['POS_tokens'].head(1).tolist()[0][:3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Inspired from https://stackoverflow.com/a/15590384\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatizing each word with its POS tag, in each sentence\n",
    "data['tokens_sentences_lemmatized'] = data['POS_tokens'].progress_map(\n",
    "    lambda list_tokens_POS: [\n",
    "        [\n",
    "            lemmatizer.lemmatize(el[0], get_wordnet_pos(el[1]))\n",
    "            if get_wordnet_pos(el[1]) != '' else el[0] for el in tokens_POS\n",
    "        ]\n",
    "        for tokens_POS in list_tokens_POS\n",
    "    ]\n",
    ")\n",
    "\n",
    "data['tokens_sentences_lemmatized'].head(1).tolist()[0][:3]\n",
    "\n",
    "# Regrouping tokens and removing stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_verbs = ['say', 'get', 'go', 'know', 'may', 'need', 'like', 'make', 'see', 'want', 'come', 'take', 'use',\n",
    "                   'would', 'can']\n",
    "stopwords_other = ['one', 'mr', 'bbc', 'image', 'getty', 'de', 'en', 'caption', 'also', 'copyright', 'something']\n",
    "my_stopwords = stopwords.words('English') + stopwords_verbs + stopwords_other\n",
    "\n",
    "from itertools import chain  # to flatten list of sentences of tokens into list of tokens\n",
    "\n",
    "data['tokens'] = data['tokens_sentences_lemmatized'].map(lambda sentences: list(chain.from_iterable(sentences)))\n",
    "data['tokens'] = data['tokens'].map(lambda tokens: [token.lower() for token in tokens if token.isalpha()\n",
    "                                                    and token.lower() not in my_stopwords and len(token) > 1])\n",
    "\n",
    "data['tokens'].head(1).tolist()[0][:30]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}